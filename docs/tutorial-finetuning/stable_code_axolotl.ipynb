{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "title: Stable Code Finetuning with Axolotl\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Code Finetuning with Axolotl\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Stability-AI/stable-code-docs/blob/main/docs/tutorial-finetuning/stable_code_axolotl.ipynb)\n",
    "\n",
    "Axolotl is a popular library for finetuning models. In this tutorial, we will use Axolotl to finetune our Stable Code 3B model on FORTRAN code to show you how to create a custom code completion model on a new programming language.\n",
    "\n",
    "This tutorial is based on the wonderful notebooks from Maxime Labonne's [llm course](https://github.com/mlabonne/llm-course/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\n",
    "assert (torch.cuda.is_available()==True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the environment set up first. We will go ahead and install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==\"2.1.2\"\n",
    "!pip install -e git+https://github.com/OpenAccess-AI-Collective/axolotl#egg=axolotl\n",
    "!pip install flash-attn==\"2.5.0\"\n",
    "!pip install deepspeed==\"0.13.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axolotl relies on a config file written in YAML to specify the model and the dataset and how to train the model. We will create a config file for our finetuning task to finetune the Stable Code 3B model on FORTRAN code using QLoRA. The dataset we are picking is from `codeparrot/github-code-clean` dataset which is a collection of code scraped from GitHub. Specifically, we will be utilizing the FORTRAN MIT licensed code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Your YAML string\n",
    "yaml_string = \"\"\"\n",
    "base_model: stabilityai/stable-code-3b\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: true\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: codeparrot/github-code-clean\n",
    "    name: FORTRAN-mit\n",
    "    type: completion\n",
    "    field: code\n",
    "dataset_prepared_path:\n",
    "val_set_size: 0.05\n",
    "output_dir: ./qlora-out\n",
    "\n",
    "adapter: qlora\n",
    "lora_model_dir:\n",
    "\n",
    "sequence_len: 1096\n",
    "sample_packing: true\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules:\n",
    "lora_target_linear: true\n",
    "lora_fan_in_fan_out:\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "mlflow_experiment_name: colab-example\n",
    "\n",
    "gradient_accumulation_steps: 1\n",
    "micro_batch_size: 1\n",
    "num_epochs: 4\n",
    "max_steps: 20\n",
    "optimizer: paged_adamw_32bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: false\n",
    "fp16: true\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: false\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch:\n",
    "saves_per_epoch:\n",
    "debug:\n",
    "deepspeed:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Convert the YAML string to a Python dictionary\n",
    "yaml_dict = yaml.safe_load(yaml_string)\n",
    "\n",
    "# Specify your file path\n",
    "file_path = 'test_axolotl.yaml'\n",
    "\n",
    "# Write the YAML file\n",
    "with open(file_path, 'w') as file:\n",
    "    yaml.dump(yaml_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can launch the run, which should take about XX if you are running this in a Colab environment with a T4 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buy using the ! the comand will be executed as a bash command\n",
    "!accelerate launch -m axolotl.cli.train /content/test_axolotl.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been finetuned, we can run it in a gradio environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buy using the ! the comand will be executed as a bash command\n",
    "!accelerate launch -m axolotl.cli.inference /content/test_axolotl.yaml \\\n",
    "    --qlora_model_dir=\"./qlora-out\" --gradio"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
