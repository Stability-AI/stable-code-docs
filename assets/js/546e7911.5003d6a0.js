"use strict";(self.webpackChunkstable_code=self.webpackChunkstable_code||[]).push([[561],{6432:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>c});var o=t(4848),a=t(8453);const l={sidebar_position:1,title:"Stable Code Finetuning with Axolotl"},i="Stable Code Finetuning with Axolotl",s={id:"tutorial-finetuning/stable_code_axolotl",title:"Stable Code Finetuning with Axolotl",description:"Axolotl is a popular library for finetuning models. In this tutorial, we",source:"@site/docs/tutorial-finetuning/stable_code_axolotl.mdx",sourceDirName:"tutorial-finetuning",slug:"/tutorial-finetuning/stable_code_axolotl",permalink:"/stable-code-docs/docs/tutorial-finetuning/stable_code_axolotl",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Stable Code Finetuning with Axolotl"},sidebar:"tutorialSidebar",previous:{title:"Tutorial - Finetuning Stable Code",permalink:"/stable-code-docs/docs/category/tutorial---finetuning-stable-code"}},r={},c=[];function d(e){const n={a:"a",code:"code",h1:"h1",img:"img",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"stable-code-finetuning-with-axolotl",children:"Stable Code Finetuning with Axolotl"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://colab.research.google.com/github/Stability-AI/stable-code-docs/blob/main/docs/tutorial-finetuning/stable_code_axolotl.ipynb",children:(0,o.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:""})})}),"\n",(0,o.jsx)(n.p,{children:"Axolotl is a popular library for finetuning models. In this tutorial, we\nwill use Axolotl to finetune our Stable Code 3B model on FORTRAN code to\nshow you how to create a custom code completion model on a new\nprogramming language."}),"\n",(0,o.jsxs)(n.p,{children:["This tutorial is based on the wonderful notebooks from Maxime Labonne\u2019s\n",(0,o.jsx)(n.a,{href:"https://github.com/mlabonne/llm-course/tree/main",children:"llm course"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\n# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\nassert (torch.cuda.is_available()==True)\n"})}),"\n",(0,o.jsx)(n.p,{children:"Let\u2019s get the environment set up first. We will go ahead and install the\nnecessary packages."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'!pip install torch=="2.1.2"\n!pip install -e git+https://github.com/OpenAccess-AI-Collective/axolotl#egg=axolotl\n!pip install flash-attn=="2.5.0"\n!pip install deepspeed=="0.13.1"\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Axolotl relies on a config file written in YAML to specify the model and\nthe dataset and how to train the model. We will create a config file for\nour finetuning task to finetune the Stable Code 3B model on FORTRAN code\nusing QLoRA. The dataset we are picking is from\n",(0,o.jsx)(n.code,{children:"codeparrot/github-code-clean"})," dataset which is a collection of code\nscraped from GitHub. Specifically, we will be utilizing the FORTRAN MIT\nlicensed code."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import yaml\n\n# Your YAML string\nyaml_string = """\nbase_model: stabilityai/stable-code-3b\nmodel_type: AutoModelForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: codeparrot/github-code-clean\n    name: FORTRAN-mit\n    type: completion\n    field: code\ndataset_prepared_path:\nval_set_size: 0.05\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 1096\nsample_packing: true\npad_to_sequence_len: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project:\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\nmlflow_experiment_name: colab-example\n\ngradient_accumulation_steps: 1\nmicro_batch_size: 1\nnum_epochs: 4\nmax_steps: 20\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: false\nfp16: true\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: false\n\nwarmup_steps: 10\nevals_per_epoch:\nsaves_per_epoch:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n\n"""\n\n# Convert the YAML string to a Python dictionary\nyaml_dict = yaml.safe_load(yaml_string)\n\n# Specify your file path\nfile_path = \'test_axolotl.yaml\'\n\n# Write the YAML file\nwith open(file_path, \'w\') as file:\n    yaml.dump(yaml_dict, file)\n'})}),"\n",(0,o.jsx)(n.p,{children:"Next, we can launch the run, which should take about XX if you are\nrunning this in a Colab environment with a T4 GPU:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Buy using the ! the comand will be executed as a bash command\n!accelerate launch -m axolotl.cli.train /content/test_axolotl.yaml\n"})}),"\n",(0,o.jsx)(n.p,{children:"Once the model has been finetuned, we can run it in a gradio\nenvironment:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Buy using the ! the comand will be executed as a bash command\n!accelerate launch -m axolotl.cli.inference /content/test_axolotl.yaml \\\n    --qlora_model_dir="./qlora-out" --gradio\n'})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var o=t(6540);const a={},l=o.createContext(a);function i(e){const n=o.useContext(l);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(l.Provider,{value:n},e.children)}}}]);